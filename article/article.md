# Семантичный поиск изображений на основе алгоритмов тематического моделирования

## ... и аннотированние изображений

Многие слышали об алгоритмах тематического моделирования, как об инструменте для анализа текстов. Он позволяет выявлять множества связанных "слов" -- темы -- по признаку частой совместной встречаемости в "документах", и, затем, каждый документ можно предствить в виде вектора, каждая координата которого содержит число(!): пропорцию, насколько данный документ содержит в себе данную тематику.

Однако оказывается, что "словами" могут выступать не только слова.
Как извлечь из изображения данные, которые можно считать "словами", связать их с текстом на естественном языке с помощью тематического моделирования на аддитивной регуляризации и построить поиск -- читайте далее.

## The Plan

 - Цель статьи
 - Введение в тематическое моделирование
   - Очень простое введение
   - Ссылки на подробности
   - Что есть что: PLSA, LDA, HDP, ARTM, в чем разница?
   - Подробнее об ARTM
     - Главное об ARTM: концепция мультимодальности
     - Ссылки на подробности об ARTM
       - Не забыть линк на специализацию
 - Майнинг данных
   - Источник данных, фильтрация мусора
   - Предподготовка текстов
   - Предподготовка изображений
   - Оговорка о формате данных для ARTM
 - Описание модели
   - Доступные модальности
   - Листинг тематик
 - О поиске
   - Поиск по любой модальности, как это?
 - Дополнительные возможности
   - Как воспользоваться мультимодальностью модели
   - Аннотирование
 - Еще кейсы применения
   - Пользователи ВК
   - Страницы в вебе
   - Научные работы
   - Категоризация
 - Подробности реализации прототипа
 - Ссылки


## Цель статьи
Целью данной статьи является не полное погружение читателя в технику тематического моделирования, а демонстрация возможностей тематического моделирования. Тема очень большая, по ней написано очень много хороших материалов, поэтому своей задачей я вижу замотивировать читателя углубиться в эту тематику .

В статье будут опущены некоторые важные детали(на важные детали будут даны ссылки): теоретическая часть будет посвящена скорее необходимым для понимания концепциям, чем истинной механике работы тематического моделирования.

## Введение в тематическое моделирование
### Очень простое введение
#### Немного терминологии

Слова(или "термы", "токены") -- мельчайшая единица, чаще всего являются нормализованными словами. Можно мнить их как столбики в таблице, задающей дискретное распределение.

Документ -- сущность, которая объединяет слова. Причем мы пользуемся моделью "сумки слов", для которой не важен порядок слов, важно только количество включений каждого слова в документ. Чаще всего документом является просто текст, но далее мы покажем, что документом может быть более сложная сущность.

Корпус -- набор документов. Иногда предполагается, что этот набор документов уже подготовлен к машинной обработке.

Словарь(корпуса) -- множество слов, которые встречались в документах корпуса. Чаще всего это не все слова, каторые встречались, потому что принято отфильтровывать слишком редкие и слишком частые неинформативные слова(стоп-слова).

Сумка слов(bag of words) -- неупорядоченный словарь, ключами являются токены, значениями количество их употреблений. Представимо в виде разреженного вектора, где каждая координата соответствует слову из словаря(см. ниже), а значение этой координаты количеству употреблений этого слова в данном документе.

Тематическое моделирование(далее просто ТМ) -- набор техник, без дополнительной разметки по корпусу выявляющий связанные одной тематикой термы.

#### Стат модель языка на пальцах

Итак, мысленный эксперимент. Представим себе автора, который пишет какой-то текст. Предположим, это проходная статья на хабр о биоисследованиях в космосе. То есть текст будет, например, состоять на 20% из инженерной темы, на 30% из биологичеческой и на 50% из общей лексики. Сделаем оговорку, что наш автор -- не очень хороший автор: вместо грамотного погружения в тему и последовательного озложения он пишет статьи иначе. Он изготовил трёхгранный "кубик" с площадью сторон 0.2 см^2, 0.3 см^2 и 0.5 см^2 соответственно и каждый раз, когда он собирается добавить новое слово в статью -- он бросает этот кубик, определяя, из какого лексического набора будет очередное слово. Определив это, он берёт новый многогранный(по количеству слов в лексическом наборе, до десятков тысяч граней) кубик(для каждого лексического набора он собственный), бросает его и получает новое слово, которое нужно добавить в текст. Такую процедуру он проделывает около 500 раз, пока не получит текст достаточно внушительной длины.
Звучит, возможно, дико, но это рабочая модель статистического подхода к лингвистике. И что самое главное(и дикое) -- она работает.

### Как *это* примерно работает
TODO(наделать картинок)

Сначала вы строите большую разреженную матрицу, по строкам которой расположены термы, а по столбцам документы. Фактически -- это просто сконкатенированные вектор-сумки-слов. Получается матрица размерности NxK, назовём её C(Corpora, корпус), где N -- количество уникальных термов в словаре, а K -- размер корпуса. И приводим эту матрицу к частотному виду -- чтобы сумма по столбцу равнялась единице, чтобы матрицу можно было интерпретировать как вероятностную.

Затем вы устанавливаете предполагаемое количество тем в вашем корпусе. Обычно, это число от 30 до 300(в моей практике), его можно установить "на глаз".

По частотной матрице корпуса C вы запускаете алгоритм, назовем его EM(Expectation-Maximization), который строит матрицу φ(фи, phi). Еще он строит матрицу τ(тета, theta), она обычно интересна только на моменте обучения, её всегда можно построить по φ, поскольку эта матрица состоит как раз из тематических векторов документов корпуса, по которому модель обучается. Поскольку основной функционал тематических моделей -- это генерация тематических векторов, мы всегда можем построить тематические вектора для любых документов, в том числе для исходных документов корпуса.

Матрица φ имеет размерность NxM, а τ -- это матрица размера MxK, где N -- количество уникальных термов в словаре, M -- количество тем, а K -- размер корпуса.
В ячейке φ[i,j] расположена вероятность принадлежности итого слова йотой теме(TODO, проверить). 
Вообще, всякий алгоритм тематического разложения старается сделать, чтобы произведение φ x τ -> C было максимально похоже на исходную матрицу с толки зрения функционала правдоподобия. 

И да, если вам кажется, что это похоже на матричное разложение, то вам не кажется: просто в тематическом моделировании мы накладываем на матрицы-разложения различные условия для того, чтобы получать интересные свойства, такие как интерпретируемость тем.

Теперь как применять модель? Мы строим вектор-сумку-слов(размерности N) для данного документа, делаем её частотной, запускаем EM-алгоритм(мы пропускаем его фазу, где он находит φ) с уже готовой матрицей φ, получаем тематический вектор размерности M, который описывает тематический состав данного документа.

Всё.
### Подробности
Читать [здесь](http://www.machinelearning.ru/wiki/images/2/22/Voron-2013-ptm.pdf). Достаточно многословно, зато вполне понятно вводят в курс дела. Рекомендую особо пристально посмотреть на [картинку](http://www.machinelearning.ru/wiki/images/2/22/Voron-2013-ptm.pdf#7), которая поясняет суть тематического моделирования.

Краткий вариант [тут](http://www.machinelearning.ru/wiki/images/3/34/Voron-viniti-23apr2013.pdf).

### Что есть что: PLSA, LDA, HDP, ARTM, в чем разница?
PLSA, LDA и HDP -- это достаточно старые и устоявшиеся алгоритмы тематического моделирования. Можно сказать, что каждый следующий это надстройка над предыдущим. Все они используют байесовский подход и понятия сопряженных распределений. Для их работы делаются некоторые предположения, которые не всегда выполняются и, поскольку в процессе конструирования этих алгоритмов берётся сложный  интеграл(благодаря сопряженным распределениям и жестким предположениям это делается относительно безболезненно), эти алгоритмы сложно адаптировать ко многим реальным условиям использования: короткие тексты, странные распределения термов, мультимодальность.

ARTM в свою очередь даёт инструмент для конструирования моделей под любую зачачу. В основе лежит идея регуляризации через KL-дивергенцию(мера похожести вероятностных распределений). Хорошая новость в том, что есть теоретически обоснованный способ сделать с помощью ARTM например модель эквивалентную LDA, а PLSA -- это базовая модель для ARTM. Фреймворк включает в себя различные регуляризаторы, которые можно добавлять в модель, чтобы добавлять модели желаемых свойств. Платим мы за это большим количеством коэфициентов(на каждый регуляризатор как минимум +1 коэффициент), теоретического обоснования для подбора которых пока нет. 

Вот [тут](http://www.machinelearning.ru/wiki/images/5/55/Voron-PTM-short.pdf) есть "рекламная" презентация про преимущества ARTM

### Подробнее об ARTM
Достаточно подробная статья находится [здесь](http://www.machinelearning.ru/wiki/images/b/bc/Voron-2015-BigARTM.pdf), но кроме того рекомендую почитать уже номного устаревшую, но более полную уже [упоминавшуюся](http://www.machinelearning.ru/wiki/images/b/bc/Voron-2015-BigARTM.pdf).
Так же нельзя не упомянуть [курс на курсере](https://www.coursera.org/learn/unsupervised-learning/home/welcome), в конце которого есть хорошее введение в тематическое моделирование с практикой.

#### Главное об ARTM: концепция мультимодальности
Давайте внимательнее взглянем на понятие модальности, что это?

У модальностей есть четкое определение, ниже я дам его подобие. Но гораздо проще это понятие просто *почувствовать*. Давайте попробуем.

Итак, модальность -- это **то, что можно подсчитать**: 4 слова, 8 человек, 15 пабликов, 16 аудиозаписей, 23 заведения, 42... вы поняли, наверняка.

Кроме того, модальность, это **то, что может случиться**: 4 слова *встретились*, 8 человек *лайкнули*, 15 пабликов *читаются*, 16 аудиозаписей *содержатся*, 23 заведения *работают на*, 42 ... 

И последнее, модальность, это **то, что к чему-то относится**: 4 слова встретились в *комментарии*, 8 человек лайкнули *пост*, 15 пабликов читаются *человеком*, 16 аудиозаписей содержатся в *плейлисте*, 23 заведения работают на *улице Ленина*, 42 ... Модальность всегда должна быть у **чего-то**: поста, плейлиста, района, человека...

В случае, если вы видите что-то, что можно подсчитать и оно к **чему-то** относится, то это модальность этого "**чего-то**".

Всё, заканчиваем метафизику. 

#### Модальность
Во-первых, элементы модальности -- это элементы какого-то конечного зафиксированного множества. Например, это множество всех слов в нормальной форме русского языка из словаря Даля. Или множество людей на какой-то конкретный момент времени. Или множество музыкальных треков. Или множество ДНС-серверов. Множества разных модальностей не пересекаются.

Во-вторых, для каждого элемента из множества определено число -- вероятность его появления в рамках данной модальности. Сумма этих чисел по всему множеству должна быть равна 1(мы всё-таки с вероятностью работаем). Если вы не знаете распределения, но имеете наборы объектов -- вы всегда можете прикинуть таблицу распределения по наблюдениям вашим любимым методом.

Поясняющая картинка от авторов инструмента и описание предмета обсуждения [здесь](http://www.machinelearning.ru/wiki/images/b/bc/Voron-2015-BigARTM.pdf#13)(описание страницей выше).

###### Примеры
Возьмём научные работы. Предположим, что для каждой статьи мы знаем список скачавших эту статью и список авторов. В таком случае, к модальности текстов добавляется еще 2 модальности: прочитавшие статью люди и написавшие статью люди. Сложно не согласиться, что такие люди определяют тематику научной статьи не хуже текста самой статьи.
![big_scheme](./modalities_schema.svg)

Либо более близкий нам пример: фотопосты в социальной сети. Одна модальность -- текстовая. Это комментарий-описание к фото. Другая модальность выделяет какие-то сущности на изображении, которые можно подсчитать: текстуры(наша первая идея), объекты(наша текущая идея). Еще одна модальность побочна и извлекается из комментариев -- это хештеги.

#### Возвращаемся к ARTM
Чем интересна ARTM -- так это своей естественной возможностью работать с любым(>1) количеством модальностей сразу. То есть с помощью ARTM можно получить не одну матрицу φ, а несколько, для каждой модальности отдельно. Причём эти матрицы будут согласованны: они будут учитываться совместно и одновременно, кроме того в хорошо натренированной модели вы даже сможете выявить взаимосвязь различных модальностей.

##### Немного о взаимосвязи модальностей

Повторю схему, которая была упомянута в примере.
![big_scheme](./modalities_schema.svg)

Представим себе свеженаписанную научную работу. У неё есть автор и текст, но пока нет читателей. Одной модальности не хватает. Но предположим, что у нас уже есть натренированная мультимодальная модель для всех трёх модальностей. В этот момент мы можем подсчитать тематический вектор научной работы v по двум имеющимся модальностям, умножить его на φ_users(как бы перейти по стрелке от вектора к пользователям) и получить вероятностный вектор, где наиболее высокие вероятности будут у пользователей, которым *интересна* научная тематика этой работы. Вот и рекомендательная система получилась.

## Переходим к практике
На текущий момент важно лишь понимание сути модальностей и механики связи модальностей через тематическое пространство

![big_scheme](./big_schema.png)
### Майнинг
#### Источник данных, фильтрация мусора
Мы скачивали открытые фотоальбомы пользователей синей российской соцсети, документом считали пару "фото-комментарий". Пользователей набирали из групп Екатеринбурга и некоторых учебных заведений.
Первая фильтрация была простой: мы выбросили все документы, где значимых токенов было менее 20.
После этого из примерно 1 000 000 записей осталось 240 000.
Беглый осмотр датасета показал, что примерно 2/3 документов -- это рекламные каталоги и прочие открытки.
Мы глазами набрали несколько сотен желательных и нежелательных записей и прогнали линейную SVM на векторах-сумках слов.
После этого осталось порядка 80 000 тысяч документов.
С ними мы работали дальше.

#### Предподготовка текстов

Сначала текст разбивался на хэштеги и остальной текст регекспом.
Затем оставшийся текст нормализовывался: выбрасывалась вся пунктуация, текст приводился к нижнему регистру.
Затем токены состоящие полностью из киррилицы лемматизировался с помощью pymorhy2(TODO линк), а всё остальное стэммилось Портером из состава NLTK(TODO линк).
На выходе из этого процесса мы получали 2 модальности: хэштеги и нормализованные токены.
Кроме того текстовый словарь фильтровался стоп-словами из корпусов NLTK


#### Предподготовка изображений
Каждое изображение векторизовывалось с помощью нейросети Inception-v3(TODO линк)
Тут надо сказать, что инцепшн из коробки нам не подходил. Это следует из стратегии и целей его обучения: он призван хорошо классифицировать изображения по 1000 классам на датасете ImageNet(TODO линк). Причем по регламенту удачным считался ответ, в топ5 выдаче которого содержится правильный ответ. То есть на выходе из нейросети мы получаем 1000-мерный очень разреженный вероятностный вектор, в котором почти всегда 99.9% вероятностной массы принадлежит одной координате. На такой выдаче нам достаточно сложно работать. Давайте посмотрим, как выглядят веса выдачи нейросети:
(TODO картинка до преобразования)

Можно обратить внимание, что около 50 координат имеют ненулевые веса. Это хорошо, это интересная особенность, давайте ею воспользуемся: попробуем "выровнять" график серией логарифмических преобразований.

(TODO код преобразований)

(TODO Картинка после преобразования)

Получилось более пристойно. В целом, это распределение всё еще достаточно сильно разрежено, но оно обладает некоторым разнообразием которое больше напоминает "слова". Остаётся лишь надеятся, что этот "шум" -- который мы вытащили, это не случайные галлюцинации нейронной сети, а просто найденные паттерны, которые сеть посчитала маловероятноми, и, в силу своего воспитания, занулила.

Эксперимент показал что идея имеет право на жизнь.

Кроме того, код этого преобразования:
```
def apply_box_cox(vector):
    v = np.asarray(vector).copy()
    v[np.where(v != 0)] = np.log(v[np.where(v != 0)])  # dont ask. some kind of box-cox transformation magic
    v[np.where(v != 0)] = v[np.where(v != 0)] / v[np.where(v != 0)].sum()
    v[np.where(v != 0)] = (1 - v[np.where(v != 0)]) / (1 - v[np.where(v != 0)]).sum()
    v[np.where(v != 0)] = v[np.where(v != 0)] - v[np.where(v != 0)].min()
    v[np.where(v != 0)] = v[np.where(v != 0)] / v[np.where(v != 0)].sum()

    v = vector.reshape(-1) + v.reshape(-1) * 3  # just help trasformation by multy3 for more stable sampling
    v = v / v.sum()
    return v
```



#### Оговорка о формате данных для ARTM

Несмотря на то, что математика позволяет(вот тут я могу ошибаться) скармливать в процесс обучения ARTM уже готовые распределения, такой ручки в используемом фреймворке нет. Поэтому, просто насемплим сотню-другую "слов" из полученного мультиномиального распределения. Получим просто коллекцию чисел: выпавших координат 1000мерного вероятностного вектора.
Это наша третья модальность.

### О модели
В силу направленности статьи на ознакомление с возможностями тематического моделирования автор опустит описание процесса обучения. Но о нём стоит сказать несколько вещей: во-первых, этот процесс не очень тривиален в силу достаточно большого количества настраиваемых параметров, во-вторых процесс трудно автоматически валидировать. Вы можете найти подробности о процессе обучения в [уже упомянутом третьем курсе специализации на Coursera](https://www.coursera.org/learn/unsupervised-learning/home/welcome).

Поэтому просто приведу ссылку на текущие характеристики модели. Люди, знакомые с фреймворком смогут посмотреть параметры модели, а незнакомые смогут просто посмотреть на состав тем и понять, смогла ли модель выделить тематики, или нет.
(TODO линк на мета-страницу)

### О поиске
То, ради чего мы тут собрались.

#### Предподготовка
Предварительно мы посчитаем тематические вектора для всех скачанных объектов и построим по ним индекс. Правильно строить индекс по векторному пространству можно с помощью annoy(простой способ)(TODO link) или nms(продвинутый способ)(TODO link), мы же обошлись простым линейным поиском, поскольку в нашем случае объектов достаточно мало.

#### Обработка запроса
Далее мы воспользуемся концепциями "документа" и тематического вектора.
Из вашего запроса мы делаем "документ", у которого заполнена одна или несколько модальностей(текст/изображение). Каждая из модальностей влияет на итоговый тематический вектор для запроса.

Затем мы посчитаем расстояние по манхеттену от вектора запроса до каждого объекта из индекса и выбираем ближайший.


#### Как сочетаются разные модальности?

Тематический вектор для документа подбирается так, чтобы максимально хорошо объяснять наблюдаемое внутри модальности документа распрдеделение. Например, набор слов в документе или пользователей, которые его посмотрели. То есть для модальности документа максимизируется правдоподобие по вектору темы. В случае нескольких модальностей мы максимизируем уже взвешенную сумму правдоподобий для каждой модальности. В случае отсутствия какой-то модальности -- мы просто зануляем этот член суммы и будем максимизировать оставшийся функционал. Как видите, тут тоже эксплуатируеруется аддитивность.

#### Вместо вывода
Как видите, мультимодальсность -- это естественное состояние ARTM. Более того, можно в любой момент добавлять и убирать модальности.

Для добавления достаточно дообучить модель. Тут есть 2 способа. 1. Вы дообучаете недостающую матрицу новой модальности и не меняете все остальные матрицы, в этом случае вы не меняя тематического пространства добавляете возможность использовать новую информацию.
2. Вы дообучаете вообще все матрицы -- модель сможет полностью адаптироваться к новой информации и уточнить старые матрицы используя новую модальность, в этом случае изменится само тематическое пространство.

Для удаления модальности вы можете занулить её вес в сумме.

Возможность взвешивания и оперативного вмешательства в модель могут помочь построить очень гибкий поиск. Возможно даже персонально подобрать веса "важности" модальности.


## Дополнительные возможности
Мы разобрали основной кейс использования тематических моделей: векторизация и понижение размерности с семантичным толкованием и разобрали одну из интересных фишек ARTM -- мультимодальность. Однако кроме прямого преобразования от наблюдаемых модальностей к тематическому вектору нам таже доступно обратное преобразование: по полученному тематическому вектору можно получить предполагаемое распределение ненаблюдаемой модальности. Зачем это может понадобиться?

### Аннотирование
Первое что приходит в голову -- аннотирование. Ведь если вы можете связать текст с каким-то другим объектом, например фото или аудио, вы можете для новых фото и аудио попробовать построить автоматическую аннотацию.

### Классификация
Это можно сделать например для научных работ. Для некоторых из них известен журнал и тематические тэги. Для тех работ, где они неизвестны, можно получить вектор с вероятностями принадлежности научной работы какому-то журналу или тематическим тегам. В результате получается так называемая "мягкая" классификация.

## Для чего еще можно использовать ARTM

 - Социальные сети: за модальности можно взять друзей, сообщества, видео и аудио файлы. Затем можно строить профиль пользователя и рекомендовать ему что-то почитать или куда-то вступить.
 - Веб-страницы: модальностями могут быть тексты, медиа-элементы, пользователи. Классификация, рекомендация, выявление трендов в блогах.
 - Научные работы: по сути как с веб-страницами.
 
 Предлагаю обсудить еще кейсы применения тематических моделей, которые вам кажутся интересными, в комментариях. 
## Подробности реализации прототипа
 
![app-schema](./app_scheme.svg)
 
В целом схема объясняет практически всё. Продублирую её текстом.

Есть 3 сервиса
 - web-service это веб-сайт(формы ввода и "обратная связь") и логика поиска
 - nn-service преобразует изображение нейронной сетью в вектор
 - artm-service предобрабатывает текст, преобразует вектор изображения, применяет модель ARTM, отдаёт тематический вектор и "аннотирует" документ всеми модальностями 

Сервисы связаны через docker-compose, причем предусмотрено 2 разных compose-файла: для нашего прода(с сертификатами) и dev-версия для отладки и развёртки локально. Вы можете развернуть копию сервиса у себя, для этого читайте инструкцию на нашем [гите](https://github.com/menshikh-iv/image2pic). (TODO кинуть ссылку на фак, "как установить")
Сервисы общаются по ZeroMQ через REQ-REP сокеты.

Логика обработки запроса примерно следующая: 
![request-scheme](./request_scheme.svg)


 ## Ссылки

Наше демо: [image2pic.tech](https://image2pic.tech). Напоминаю, что это лишь прототип, не ломайте его специально, пожалуйста.

Наш github: [image2pic](https://github.com/menshikh-iv/image2pic)


[BigARTM](http://bigartm.org/) - Публикации: [1](https://github.com/bigartm/bigartm/wiki/Publications), [2](http://www.machinelearning.ru/wiki/index.php?title=BigARTM) об ARTM

[Inception-v3 keras-based implementation](https://keras.io/applications/)