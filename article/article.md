# Семантичный поиск изображений на основе алгоритмов тематического моделирования

## ... и аннотированние изображений

Многие слышали об алгоритмах тематического моделирования, как об инструменте для анализа текстов. Он позволяет выявлять множества связанных "слов" -- темы -- по признаку частой совместной встречаемости в "документах", и, затем, каждый документ можно предствить в виде вектора, каждая координата которого содержит число(!): пропорцию, насколько данный документ содержит в себе данную тематику.

Однако оказывается, что "словами" могут выступать не только слова.
Как извлечь из изображения данные, которые можно считать "словами", связать их со словами на естественном языке с помощью тематического моделирования на аддитивной регуляризации и построить поиск -- читайте далее.

## The Plan

 - Цель статьи
 - Введение в тематическое моделирование
   - Очень простое введение
   - Ссылки на подробности
   - Что есть что: PLSA, LDA, HDP, ARTM, в чем разница?
   - Подробнее об ARTM
     - Главное об ARTM: концепция мультимодальности
     - Ссылки на подробности об ARTM
       - Не забыть линк на специализацию
 - Майнинг данных
   - Источник данных, фильтрация мусора
   - Предподготовка текстов
   - Предподготовка изображений
   - Оговорка о формате данных для ARTM
 - Описание модели
   - Доступные модальностиВнимание на схему ниже.
   - Листинг тематик
 - О поиске
   - Поиск по любой модальности, как это?
 - Дополнительные возможности
   - Как воспользоваться мультимодальностью модели
   - Аннотирование
 - Еще кейсы применения
   - Пользователи ВК
   - Страницы в вебе
   - Научные работы
   - Категоризация
 - Спасибо за внимание, предлагайте кейсы применения мультимодальных моделейных моделей
 - Ссылки
   - ссылка на демо(не шатайте его сильно, пожалуйста)
   - ссылка на BigARTM
   - ссылка на керас


## Цель статьи
Целью данной статьи является не полное погружение читателя в технику тематического моделирования, а демонстрация возможностей тематического моделирования. Тема очень большая, по ней написано очень много хороших материалов, поэтому своей задачей я вижу замотивировать читателя углубиться в эту тематику.

Поэтому в статье будут опущены некоторые важные детали(на важные детали будут даны ссылки): теоретическая часть будет посвящена скорее необходимым для понимания концепциям, чем истинной механике работы тематического моделирования.

## Введение в тематическое моделирование
### Очень простое введение
#### Немного терминологии
Слова(или "термы", "токены") -- мельчайшая единица, чаще всего являются нормализованными словами. Можно мнить их как столбики в таблице, задающей дискретное распределение.

Сумка слов(bag of words) -- неупорядоченный словарь, ключами являются токены, значениями количество их употреблений.

Документ -- сущность, которая объединаяет слова. Причем мы пользуемся моделью "сумки слов", для которой не важен порядок слов, важно только количество включений каждого слова в документ. Чаще всего документом является просто текст, но далее мы покажем, что документом может быть более сложная сущность.

Корпус -- набор документов. Иногда предполагается, что этот набор документов уже подготовлен к машинной обработке.

Тематическое моделирование(далее просто ТМ) -- набор техник, без дополнительной разметки по корпусу выявляющий связанные одной тематикой термы.

### Как это примерно работает

Сначала вы устанавливаете предполагаемое количество тем в вашем корпусе. Обычно, это число от 30 до 300(в моей практике), его можно установить "на глаз".
По корпусу вы запускаете алгоритм, который строит матрицу фи(phi, TODO воткнуть греческую фи). Еще он строит матрицу тета, но сегодня она нам неинтересна.
Матрица фи имеет размерность NxM, где N -- количество уникальных слов в словаре, а M -- количество тем.
В ячейке phi[i,j] расположена вероятность принадлежности итого слова йотой теме(TODO, проверить).

Теперь очень понятно, как дальше применять эту матрицу фи: мы строим вектор-сумку-слов(размерности N) для данного документа, умножаем его на матрицу фи(размерности NxM), получаем тематический вектор размерности(M), который описывает тематический состав данного документа. (TODO исправить. Это неправда)

Всё.
### Подробности
Читать [здесь](http://www.machinelearning.ru/wiki/images/2/22/Voron-2013-ptm.pdf). Достаточно многословно, зато вполне понятно вводят в курс дела. Рекомендую особо пристально посмотреть на [картинку](http://www.machinelearning.ru/wiki/images/2/22/Voron-2013-ptm.pdf#7), которая поясняет суть тематического моделирования.

Краткий вариант [тут](http://www.machinelearning.ru/wiki/images/3/34/Voron-viniti-23apr2013.pdf).

### Что есть что: PLSA, LDA, HDP, ARTM, в чем разница?
PLSA, LDA и HDP -- это достаточно старые и устоявшиеся алгоритмы тематического моделирования. Можно сказать, что каждый следующий это надстройка над предыдущим. Все они используют байесовский подход и понятия сопряженных распределений. Для их работы делаются некоторые предположения, которые не всегда выполняются и, поскольку в процессе конструирования этих алгоритмов берётся сложный  интеграл(благодаря сопряженным распределениям и жестким предположениям это делается относительно безболезненно), эти алгоритмы сложно адаптировать ко многим реальным условиям использования: короткие тексты, странные распределения термов, мультимодальность.

ARTM в свою очередь даёт инструмент для конструирования моделей под любую зачачу. В основе лежит идея регуляризации через KL-дивергенцию(мера похожести вероятностных распределений). Фреймворк включает в себя различные регуляризаторы, которые можно добавлять в модель, чтобы добавлять модели желаемых свойств. Платим мы за это большим количеством коэфициентов, теоретического обоснования для подбора которых пока нет.

Вот [тут](http://www.machinelearning.ru/wiki/images/5/55/Voron-PTM-short.pdf) есть "рекламная" презентация про преимущества ARTM

### Подробнее об ARTM
Достаточно подробная статья находится [здесь](http://www.machinelearning.ru/wiki/images/b/bc/Voron-2015-BigARTM.pdf), но кроме того рекомендую почитать уже номного устаревшую, но более полную уже [упоминавшуюся](http://www.machinelearning.ru/wiki/images/b/bc/Voron-2015-BigARTM.pdf).
Так же нельзя не упомянуть [курс на курсере](https://www.coursera.org/learn/unsupervised-learning/home/welcome), в конце которого есть хорошее введение в тематическое моделирование с практикой.

#### Главное об ARTM: концепция мультимодальности
Давайте внимательнее взглянем на понятие модальности, что это?

Давайте попробуем понять это на примере модели языка.
Итак мысленный эксперимент. Представим себе автора, который пишет какой-то текст. Предположим, это проходная статья на хабр о биоисследованиях в космосе. То есть текст будет состоять на 20% из инженерной темы, на 30% из биологичеческой и на 50% из общей лексики. Сделаем оговорку, что наш автор -- не очень хороший автор: вместо грамотного погружения в тему и последовательного озложения он пишет статьи иначе. Он изготовил трёхгранный "кубик" с площадью сторон 0.2 см^2, 0.3 см^2 и 0.5 см^2 соответственно и каждый раз, когда он собирается добавить новое слово в статью -- он бросает этот кубик, определяя, из какого лексического набора будет очередное слово. Определив это, он берёт новый многогранный(по количеству слов в лексическом наборе, до десятков тысяч граней) кубик(для каждого лексического набора он собственный), бросает его и получает новое слово, которое нужно добавить в текст. Такую процедуру он проделывает около 500 раз, пока не получит текст достаточно внушительной длины.
Звучит, возможно, дико, но это рабочая модель статистического подхода к лингвистике. И что самое главное(и дикое) -- она работает.

#### Модальность
Во-первых, элементы модальности -- это элементы какого-то конечного зафиксированного множества. Например, это множество всех слов в нормальной форме русского языка из словаря Даля. Или множество людей на какой-то конкретный момент времени. Или множество музыкальных треков. Или множество ДНС-серверов.

Во-вторых, для каждого элемента из множества определено число -- вероятность его появления в рамках данной модальности. Сумма этих чисел по всему множеству должна быть равна 1(мы всё-таки с вероятностью работаем).

##### Зачем называть это модальностью? Почему не назвать просто словами?
А это самое интересное. До сих пор мы приводили примеры документов, которые содержат только одну модальность -- токены. Но что если пойти дальше?

###### Пример
Возьмём научные работы. Предположим, что для каждой статьи мы знаем список скачавших эту статью и список авторов. В таком случае, к модальности текстов добавляется еще 2 модальности: прочитавшие статью люди и написавшие статью люди. Сложно не согласиться, что такие люди определяют тематику научной статьи не хуже текста самой статьи.
![big_scheme](./modalities_schema.png)

#### Возвращаемся к ARTM
Чем интересна ARTM -- так это своей естественной возможностью работать с любым количеством модальностей сразу. То есть с помощью ARTM можно получить не одну матрицу фи, а несколько, для каждой модальности отдельно. Причём эти матрицы будут согласованны.

TODO согласованность -- это как? Показать наглядно преобразования.

## Переходим к практике
На текущий момент важно лишь понимание сути модальностей и механики связи модальностей через тематическое пространство

![big_scheme](./big_schema.png)
### Майнинг
#### Источник данных, фильтрация мусора
Мы скачивали открытые фотоальбомы пользователей синей российской соцсети, документом считали пару "фото-комментарий". Пользователей набирали из групп Екатеринбурга и некоторых учебных заведений.
Первая фильтрация была простой: мы выбросили все документы, где значимых токенов было менее 20.
После этого из примерно 1 000 000 записей осталось 240 000.
Беглый осмотр датасета показал, что примерно 2/3 документов -- это рекламные каталоги и прочие открытки.
Мы глазами набрали несколько сотен желательных и нежелательных записей и прогнали линейную SVM на векторах-сумках слов.
После этого осталось порядка 80 000 тысяч документов.
С ними мы работали дальше.

#### Предподготовка текстов

Сначала текст разбивался на хэштеги и остальной текст регекспом.
Затем оставшийся текст нормализовывался: выбрасывалась вся пунктуация, текст приводился к нижнему регистру.
Затем токены состоящие полностью из киррилицы лемматизировался с помощью pymorhy2(TODO линк), а всё остальное стэммилось Портером из состава NLTK(TODO линк).
На выходе из этого процесса мы получали 2 модальности: хэштеги и нормализованные токены.


#### Предподготовка изображений
Каждое изображение векторизовывалось с помощью нейросети Inception-v3(TODO линк)
Тут надо сказать, что инцепшн из коробки нам не подходил. Это следует из стратегии и целей его обучения: он призван хорошо классифицировать изображения по 1000 классам на датасете ImageNet(TODO линк). Причем по регламенту удачным считался ответ, в топ5 выдаче которого содержится правильный ответ. То есть на выходе из нейросети мы получаем 1000-мерный очень разреженный вероятностный вектор, в котором почти всегда 99.9% вероятностной массы принадлежит одной координате. На такой выдаче нам достаточно сложно работать. Давайте посмотрим, как выглядят веса выдачи нейросети:
(TODO картинка до преобразования)

Можно обратить внимание, что около 50 координат имеют ненулевые веса. Это хорошо, это интересная особенность, давайте ею воспользуемся: попробуем "выровнять" график серией логарифмических преобразований.

(TODO код преобразований)

(TODO Картинка после преобразования)

Получилось более пристойно. В целом, это распределение всё еще достаточно сильно разрежено, но оно обладает некоторым разнообразием которое больше напоминает "слова". Остаётся лишь надеятся, что этот "шум" -- который мы вытащили, это не случайные галлюцинации нейронной сети, а просто найденные паттерны, которые сеть посчитала маловероятноми, и, в силу своего воспитания, занулила.

Эксперимент показал что идея имеет право на жизнь.


#### Оговорка о формате данных для ARTM

Несмотря на то, что математика позволяет(вот тут я могу ошибаться) скармливать в процесс обучения ARTM уже готовые распределения, такой ручки в используемом фреймворке нет. Поэтому, просто насемплим сотню-другую "слов" из полученного мультиномиального распределения. Получим просто коллекцию чисел: выпавших координат 1000мерного вероятностного вектора.
Это наша третья модальность.

### О модели
В силу направленности статьи на ознакомление с возможностями тематического моделирования автор опустит описание процесса обучения. Но о нём стоит сказать несколько вещей: во-первых, этот процесс не очень тривиален в силу достаточно большого количества настраиваемых параметров, во-вторых процесс трудно автоматически валидировать. Вы можете найти подробности о процессе обучения в [уже упомянутом третьем курсе специализации на Coursera](https://www.coursera.org/learn/unsupervised-learning/home/welcome).

Поэтому просто приведу ссылку на текущие характеристики модели. Люди, знакомые с фреймворком смогут посмотреть параметры модели, а незнакомые смогут просто посмотреть на состав тем и понять, смогла ли модель выделить тематики, или нет.
(TODO линк на мета-страницу)

### О поиске
То, ради чего мы тут собрались.

#### Предподготовка
Предварительно мы посчитаем тематические вектора для всех скачанных объектов и построим по ним индекс. Правильно строить индекс по векторному пространству можно с помощью annoy(простой способ)(TODO link) или nms(продвинутый способ)(TODO link), мы же обошлись простым линейным поиском, поскольку в нашем случае объектов достаточно мало.

#### Обработка запроса
Далее мы воспользуемся концепциями "документа" и тематического вектора.
Из вашего запроса мы делаем "документ", у которого заполнена одна или несколько модальностей(текст/изображение). Каждая из модальностей влияет на итоговый тематический вектор для запроса.

Затем мы посчитаем расстояние по манхеттену от вектора запроса до каждого объекта из индекса и выбираем ближайший.


#### Как сочетаются разные модальности?

Тематический вектор для документа подбирается так, чтобы максимально хорошо объяснять наблюдаемое внутри модальности документа распрдеделение. Например, набор слов в документе или пользователей, которые его посмотрели. То есть для модальности документа максимизируется правдоподобие по вектору темы. В случае нескольких модальностей мы максимизируем уже взвешенную сумму правдоподобий для каждой модальности. В случае отсутствия какой-то модальности -- мы просто зануляем этот член суммы и будем максимизировать оставшийся функционал. Как видите, тут тоже эксплуатируеруется аддитивность.

#### Вместо вывода
Как видите, мультимодальсность -- это естественное состояние ARTM. Более того, можно в любой момент добавлять и убирать модальности.

Для добавления достаточно дообучить модель. Тут есть 2 способа. 1. Вы дообучаете недостающую матрицу новой модальности и не меняете все остальные матрицы, в этом случае вы не меняя тематического пространства добавляете возможность использовать новую информацию.
2. Вы дообучаете вообще все матрицы -- модель сможет полностью адаптироваться к новой информации и уточнить старые матрицы используя новую модальность, в этом случае изменится само тематическое пространство.

Для удаления модальности вы можете занулить её вес в сумме.

Возможность взвешивания и оперативного вмешательства в модель могут помочь построить очень гибкий поиск. Возможно даже персонально подобрать веса "важности" модальности.

https://image2pic.tech/

## Дополнительные возможности
Мы разобрали основной кейс использования тематических моделей: векторизация и понижение размерности с семантичным толкованием и разобрали одну из интересных фишек ARTM -- мультимодальность. Однако кроме прямого преобразования от наблюдаемых модальностей к тематическому вектору нам таже доступно обратное преобразование: по полученному тематическому вектору можно получить предполагаемое распределение ненаблюдаемой модальности. Зачем это может понадобиться?

### Аннотирование
Первое что приходит в голову -- аннотирование. Ведь если вы можете связать текст с каким-то другим объектом, например фото или аудио, вы можете для новых фото и аудио попробовать построить автоматическую аннотацию.

### Классификация
Это можно сделать например для научных работ. Для некоторых из них известен журнал и тематические тэги. Для тех работ, где они неизвестны, можно получить вектор с вероятностями принадлежности научной работы какому-то журналу или тематическим тегам. В результате получается так называемая "мягкая" классификация.

    - Еще кейсы применения
      - Пользователи ВК
      - Страницы в вебе
      - Научные работы
      - Категоризация
    - Спасибо за внимание, предлагайте кейсы применения мультимодальных моделейных моделей
    - Ссылки
      - ссылка на демо(не шатайте его сильно, пожалуйста)
      - ссылка на BigARTM
      - ссылка на керас
