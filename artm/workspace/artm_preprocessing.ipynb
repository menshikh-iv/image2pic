{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, io, h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import artm\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import sample_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/insception-classes.tsv') as income:\n",
    "    class_names = dict(map(lambda l: map(str.strip, l.split('\\t')), income))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/second_big_cleaned_andlenght-filtered.json') as income:\n",
    "    df = pd.DataFrame(map(json.loads, income))\n",
    "\n",
    "with h5py.File(\"../data/img_url2inception.backup.h5\", 'r') as hdf5_inception_dreams:\n",
    "    %time df['classes'] = df.img_url.apply(hdf5_inception_dreams.get).apply(np.array) # Aware! Random disc access!\n",
    "\n",
    "df.dropna(axis=0, subset=['classes'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1., random_state=42) # some shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_pics = df.iloc[:samples_size]\n",
    "only_texts = df.iloc[samples_size:2*samples_size]\n",
    "df = df.iloc[2*samples_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_pics.drop('tag', axis=1, inplace=True)\n",
    "only_pics.drop('text', axis=1, inplace=True)\n",
    "only_pics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_texts.drop('classes', axis=1, inplace=True)\n",
    "only_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "sns.plt.scatter(np.log(df.classes[idx]), zip(*sample_from(df.classes[idx]).items())[-1])\n",
    "sns.plt.xlabel('prob')\n",
    "sns.plt.ylabel('times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in range (3):\n",
    "    sns.barplot(zip(*sample_from(df.classes[idx]).items())[0], zip(*sample_from(df.classes[idx]).items())[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.text = df.text.apply(Counter)\n",
    "only_texts.text = only_texts.text.apply(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tag = df.tag.apply(Counter)\n",
    "only_texts.tag = only_texts.tag.apply(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.classes = df.classes.apply(sample_from)\n",
    "only_pics.classes = only_pics.classes.apply(sample_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "once_awared = set()\n",
    "\n",
    "\n",
    "def to_vw(row, key='img_url', fields=['tag', 'text', 'classes']):\n",
    "    line = '%s '%row[key][len('https://'):]\n",
    "        \n",
    "    for field in fields:\n",
    "        \n",
    "        if field not in row:\n",
    "            if field not in once_awared:\n",
    "                print 'WARNING: there is no %s field in the row %s'%(field, row[key])\n",
    "                once_awared.add(field)\n",
    "            continue\n",
    "        \n",
    "        if row[field] is not None and len(row[field]) > 0:\n",
    "            line += '|%s ' % field\n",
    "            line += '%s '%' '.join('%s:%i'%(unicode(pair[0]).replace(':', ''), pair[-1]) for pair in row[field].items() if pair[-1]>0 and len(unicode(pair[0]).replace(':', ''))>0)\n",
    "    return '%s\\n'%line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = '68743_of_tags_text_classes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = '17424_of_tags_text_classes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'to_filter_of_tags_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_artm(df, dataset_name):\n",
    "    vw_path = '../data/%s.vw'%dataset_name\n",
    "    with io.open(vw_path, 'w', encoding='utf8') as outcome:\n",
    "        for k, row in df.iterrows():\n",
    "            outcome.write(to_vw(row))\n",
    "    artm_path = '../data/%s_batches'%dataset_name\n",
    "    artm.BatchVectorizer(target_folder=artm_path, data_path=vw_path, data_format='vowpal_wabbit', batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_for_artm(df, 'main_trainset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_for_artm(only_texts, 'only_text_testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_for_artm(only_pics, 'only_pics_testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'main_trainset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artm_path = '../data/%s_batches'%dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_to_train = artm.BatchVectorizer(data_path=artm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чищу словарь. Закройте глаза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# from pymorphy2 import MorphAnalyzer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "rus_stop_words = set(stopwords.words('russian'))\n",
    "en_stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_to_train.dictionary.filter('text', min_df=2, max_df_rate=0.5)\n",
    "batch_to_train.dictionary.filter('tag', min_df=2)\n",
    "batch_to_train.dictionary.filter('classes', min_df=2, max_df_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_to_train.dictionary.save_text('../data/dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artm_dict = pd.read_csv('../data/dict.csv', skiprows=1, sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artm_dict.token = artm_dict.token.apply(lambda w: unicode(w.decode('utf8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "ru_stemmer = RussianStemmer()\n",
    "\n",
    "print artm_dict.shape\n",
    "for stoplist in [map(stemmer.stem, en_stop_words)]:\n",
    "    artm_dict = artm_dict[~artm_dict.token.isin(stoplist)]\n",
    "    \n",
    "print artm_dict.shape\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "stoplist = set([ru_stemmer.stem(tok) for tok in rus_stop_words])\n",
    "artm_dict = artm_dict[~artm_dict.token.isin(stoplist)]\n",
    "\n",
    "print artm_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artm_dict = artm_dict[~((artm_dict.class_id == 'text') & (artm_dict.token.str.len()<2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_path = '../data/dict.filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artm_dict.to_csv(filtered_path, encoding='utf8', index=None, header=True)\n",
    "\n",
    "! echo \"name: en_ru_stops_filtered\" > /tmp/header_of_dict\n",
    "! cp $filtered_path /tmp/full.dict\n",
    "! cat /tmp/full.dict | sed -ne 's/,\\(\\w\\)/, \\1/gp' > /tmp/full.dict.filtered\n",
    "! cat /tmp/header_of_dict /tmp/full.dict.filtered > $filtered_path\n",
    "\n",
    "! rm /tmp/full.dict.filtered /tmp/full.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_dict = artm.Dictionary()\n",
    "cleaned_dict.load_text(filtered_path if filtered_path is not None else '../data/dict.filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparsed_topics = 39\n",
    "smoothed_topics = 5\n",
    "\n",
    "topics = ['good_%i'%_ for _ in range(sparsed_topics)] + ['mess_%s'%_ for _ in range(smoothed_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm = artm.ARTM(topic_names=topics, num_processors=2)\n",
    "\n",
    "tm.initialize(cleaned_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm.regularizers.add(artm.SmoothSparsePhiRegularizer(name='text_sparser', tau=-0.7, class_ids=['text']))\n",
    "\n",
    "tm.regularizers.add(artm.SmoothSparsePhiRegularizer(name='classes_sparser', tau=-0.3, class_ids=['classes']))\n",
    "\n",
    "tm.regularizers.add(artm.SmoothSparsePhiRegularizer(name='tags_smoother', tau=1, class_ids=['tag']))\n",
    "\n",
    "tm.regularizers.add(artm.SmoothSparseThetaRegularizer(name='topic_sparser', tau=-2, topic_names=topics[:sparsed_topics]))\n",
    "\n",
    "tm.regularizers.add(artm.SmoothSparseThetaRegularizer(name='topic_smoother', tau=2.5, topic_names=topics[smoothed_topics:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm.scores.add(artm.PerplexityScore(name='perp', class_ids=['text', 'tag', 'classes']))\n",
    "\n",
    "tm.scores.add(artm.TopTokensScore(name='top_words', class_id='text', num_tokens=250))\n",
    "\n",
    "tm.scores.add(artm.TopTokensScore(name='top_tags', class_id='tag'))\n",
    "\n",
    "tm.scores.add(artm.TopTokensScore(name='top_classes', class_id='classes', num_tokens=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time tm.fit_offline(batch_to_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm.regularizers.add(artm.TopicSelectionThetaRegularizer(name='selector', tau=0.4, topic_names=topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for _ in range(10):\n",
    "    tm.regularizers['selector'].tau +=0.02\n",
    "    tm.fit_offline(batch_to_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for _ in range(4):\n",
    "    tm.regularizers['selector'].tau +=0.05\n",
    "    tm.fit_offline(batch_to_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time tm.fit_offline(batch_to_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm.regularizers['selector'].tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARTM training end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perplexity_score = tm.score_tracker['perp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.plt.plot(perplexity_score.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_weighted(modality_score_tracker):\n",
    "    res = {}\n",
    "    \n",
    "    lst_tokens = modality_score_tracker.last_tokens\n",
    "    lst_weights = modality_score_tracker.last_weights\n",
    "    \n",
    "    for k in sorted(lst_tokens.keys()):\n",
    "        res[k] = [(word, v) for (word, v) in \n",
    "                  zip(lst_tokens[k], lst_weights[k]) \n",
    "                      if v > 0]\n",
    "    topics, weights = zip(*res.items())\n",
    "    weights = map(dict, weights)\n",
    "    return pd.DataFrame(weights, index=topics).fillna(0).sort_index()\n",
    "\n",
    "def get_top(row, treshold=0.95, number=5):\n",
    "    sorted_row = row.sort_values(ascending=False)\n",
    "    \n",
    "    res = []\n",
    "    prob_mass = 0\n",
    "    for k, val in sorted_row.iteritems():\n",
    "        prob_mass+=val\n",
    "        res.append((k, val))\n",
    "        if prob_mass>treshold:\n",
    "            break\n",
    "        if len(res)==number:\n",
    "            res.append(('other', (1. - prob_mass)))\n",
    "            break\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_modality_top(mod_name, subst_name=None, thresold=0.95, max_items=5, need_print = False):\n",
    "    topic_words = get_weighted(tm.score_tracker[mod_name])\\\n",
    "                                .apply(lambda r: get_top(r, thresold, max_items), axis=1)\n",
    "    if need_print:\n",
    "        for t, words in topic_words.iteritems():\n",
    "            print '%s: %s'%(t, ',\\t'.join(\n",
    "                '%s:%3.3f'%(name if subst_name is None else subst_name.get(name, 'empty_subst'), value)\n",
    "                                            for name, value in words if value>0))\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_modality_top('top_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_modality_top('top_words', max_items=15, need_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_modality_top('top_classes', subst_name=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm.save('../data/big_model.artm.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_modality_mat(mat, name, ):\n",
    "    mat.to_csv('../data/%s.csv'%name, sep = '\\t', encoding='utf8')\n",
    "\n",
    "    with io.open('../data/%s.json'%name, 'w', encoding='utf8') as outcome:\n",
    "        for u, data in mat.iterrows():\n",
    "            ans = dict((k, float(v)) for k,v in data.to_dict().items())\n",
    "            ans['token'] = u\n",
    "            outcome.write(u'%s\\n'% json.dumps(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_mat_text = tm.get_phi(topic_names=topics, class_ids=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(phi_mat_text.sum(axis=1) < 10**-9).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_mat_text.sum() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_modality_mat(phi_mat_text, 'phi_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_mat_classes = tm.get_phi(topic_names=topics, class_ids=['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_modality_mat(phi_mat_classes, 'phi_classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_mat_tag = tm.get_phi(topic_names=topics, class_ids=['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_modality_mat(phi_mat_tag, 'phi_tag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Дальше лучше не читать, оно там просто валяется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_pic_batch = artm.BatchVectorizer(data_path='../data/only_pics_testset_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_text_batch = artm.BatchVectorizer(data_path='../data/only_text_testset_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_pic_topics = tm.transform(only_pic_batch, predict_class_id='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _,v in only_pic_topics.apply(lambda x: get_top(x), axis=0).iteritems():\n",
    "    print ', '.join('-'.join(map(unicode, r)) for r in v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(index=df.img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm2 = artm.ARTM(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm2.load('../data/big_model.artm.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm2.regularizers.add(artm.SmoothSparsePhiRegularizer(name='text_sparser', tau=-0.7, class_ids=['text']))\n",
    "tm2.regularizers.add(artm.SmoothSparsePhiRegularizer(name='classes_sparser', tau=-0.3, class_ids=['classes']))\n",
    "tm2.regularizers.add(artm.SmoothSparsePhiRegularizer(name='tags_smoother', tau=1, class_ids=['tag']))\n",
    "tm2.regularizers.add(artm.SmoothSparseThetaRegularizer(name='topic_sparser', tau=-2, topic_names=topics[:sparsed_topics]))\n",
    "tm2.regularizers.add(artm.SmoothSparseThetaRegularizer(name='topic_smoother', tau=2.5, topic_names=topics[smoothed_topics:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "badset = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goodset = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "very_bad_words = [u'арт', u'артикул', u'app2255775', u'app4216068']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in very_bad_words:\n",
    "    print 'for', word, 'there', df.text.apply(lambda l: word in l).sum(), 'records'\n",
    "    badset.update(df[df.text.apply(lambda l: word in l)].sample(frac=0.01, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_words = [u'продать', u'продажа', u'склад', u'товар', u'прокат', u'ретушь', u'сантиметр']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in bad_words:\n",
    "    print 'for', word, 'there', df.text.apply(lambda l: word in l).sum(), 'records'\n",
    "    badset.update(df[df.text.apply(lambda l: word in l)].sample(100, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_words = [u'замок', u'дружба', u'семья', u'дружба', \n",
    "              u'работа', u'поехать', u'отель', u'остров', \n",
    "              u'гулять', u'сдать', u'повезти', u'плавать',\n",
    "              u'восхитительно', u'смеяться', u'уехать']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in good_words:\n",
    "    print 'for', word, 'there', df.text.apply(lambda l: word in l).sum(), 'records'\n",
    "    goodset.update(df[df.text.apply(lambda l: word in l)].sample(frac=0.1, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(goodset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(badset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear', probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics_to_filter.shape\n",
    "\n",
    "topics_to_filter = tm.transform(batch_to_predict).T\n",
    "\n",
    "topics_to_filter.sort_index(inplace=True)\n",
    "\n",
    "topics_to_filter.index = df.img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_to_filter = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = topics_to_filter.loc[goodset].append(topics_to_filter.loc[badset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['is_ad'] = [0]*len(goodset)+[1]*len(badset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_df.drop('is_ad', axis=1), train_df.is_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(topics_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds.shape[0] - (preds).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/for_ivan_', 'w') as outcome:\n",
    "    for url in topics_to_filter[preds < 0.5].index:\n",
    "        outcome.write('%s\\n'%url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('../data/big_to_download.json', 'w', encoding='utf8') as outcome:\n",
    "    for u, data in df[preds < 0.5].iterrows():\n",
    "        ans = data.to_dict()\n",
    "        ans['img_url'] = u\n",
    "        outcome.write(u'%s\\n'% json.dumps(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
