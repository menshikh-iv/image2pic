# Семантичный поиск изображений на основе алгоритмов тематического моделирования

## ... и аннотированние изображений

Многие слышали об алгоритмах тематического моделирования, как об инструменте для анализа текстов. Он позволяет выявлять множества связанных "слов" -- темы -- по признаку частой совместной встречаемости в "документах", и, затем, каждый документ можно предствить в виде вектора, каждая координата которого содержит число(!): пропорцию, насколько данный документ содержит в себе данную тематику.

Однако оказывается, что "словами" могут выступать не только слова.
Как извлечь из изображения данные, которые можно считать "словами", связать их со словами на естественном языке с помощью тематического моделирования на аддитивной регуляризации и построить поиск -- читайте далее.

## The Plan

 - Цель статьи
 - Введение в тематическое моделирование
   - Очень простое введение
   - Ссылки на подробности
   - Что есть что: PLSA, LDA, HDP, ARTM, в чем разница?
   - Подробнее об ARTM
     - Главное об ARTM: концепция мультимодальности
     - Ссылки на подробности об ARTM
       - Не забыть линк на специализацию
 - Майнинг данных
   - Источник данных, фильтрация мусора
   - Предподготовка текстов
   - Предподготовка изображений
   - Оговорка о формате данных для ARTM
 - Описание модели
   - Доступные модальностиВнимание на схему ниже.
   - Листинг тематик
 - О поиске
   - Поиск по любой модальности, как это?
 - Дополнительные возможности
   - Как воспользоваться мультимодальностью модели
   - Аннотирование
 - Еще кейсы применения
   - Пользователи ВК
   - Страницы в вебе
   - Научные работы
   - Категоризация
 - Спасибо за внимание, предлагайте кейсы применения мультимодальных моделейных моделей
 - Ссылки
   - ссылка на демо(не шатайте его сильно, пожалуйста)
   - ссылка на BigARTM
   - ссылка на керас


## Цель статьи
Целью данной статьи является не полное погружение читателя в технику тематического моделирования, а демонстрация возможностей тематического моделирования. Тема очень большая, по ней написано очень много хороших материалов, поэтому своей задачей я вижу замотивировать читателя углубиться в эту тематику.

Поэтому в статье будут опущены некоторые важные детали(на важные детали будут даны ссылки): теоретическая часть будет посвящена скорее необходимым для понимания концепциям, чем истинной механике работы тематического моделирования.

## Введение в тематическое моделирование
### Очень простое введение
#### Немного терминологии
Слова(или "термы", "токены") -- мельчайшая единица, чаще всего являются нормализованными словами. Можно мнить их как столбики в таблице, задающей дискретное распределение.

Сумка слов(bag of words) -- неупорядоченный словарь, ключами являются токены, значениями количество их употреблений.

Документ -- сущность, которая объединаяет слова. Причем мы пользуемся моделью "сумки слов", для которой не важен порядок слов, важно только количество включений каждого слова в документ. Чаще всего документом является просто текст, но далее мы покажем, что документом может быть более сложная сущность.

Корпус -- набор документов. Иногда предполагается, что этот набор документов уже подготовлен к машинной обработке.

Тематическое моделирование(далее просто ТМ) -- набор техник, без дополнительной разметки по корпусу выявляющий связанные одной тематикой термы.

### Как это примерно работает

Сначала вы устанавливаете предполагаемое количество тем в вашем корпусе. Обычно, это число от 30 до 300(в моей практике), его можно установить "на глаз".
По корпусу вы запускаете алгоритм, который строит матрицу фи(phi, todo воткнуть греческую фи). Еще он строит матрицу тета, но сегодня она нам неинтересна.
Матрица фи имеет размерность NxM, где N -- количество уникальных слов в словаре, а M -- количество тем.
В ячейке phi[i,j] расположена вероятность принадлежности итого слова йотой теме(туду, проверить).

Теперь очень понятно, как дальше применять эту матрицу фи: мы строим вектор-сумку-слов(размерности N) для данного документа, умножаем его на матрицу фи(размерности NxM), получаем тематический вектор размерности(M), который описывает тематический состав данного документа.

Всё.
### Подробности
Читать [здесь](http://www.machinelearning.ru/wiki/images/2/22/Voron-2013-ptm.pdf). Достаточно многословно, зато вполне понятно вводят в курс дела. Рекомендую особо пристально посмотреть на [картинку](http://www.machinelearning.ru/wiki/images/2/22/Voron-2013-ptm.pdf#7), которая поясняет суть тематического моделирования.

Краткий вариант [тут](http://www.machinelearning.ru/wiki/images/3/34/Voron-viniti-23apr2013.pdf).

### Что есть что: PLSA, LDA, HDP, ARTM, в чем разница?
PLSA, LDA и HDP -- это достаточно старые и устоявшиеся алгоритмы тематического моделирования. Можно сказать, что каждый следующий это надстройка над предыдущим. Все они используют байесовский подход и понятия сопряженных распределений. Для их работы делаются некоторые предположения, которые не всегда выполняются и, поскольку в процессе конструирования этих алгоритмов берётся сложный  интеграл(благодаря сопряженным распределениям и жестким предположениям это делается относительно безболезненно), эти алгоритмы сложно адаптировать ко многим реальным условиям использования: короткие тексты, странные распределения термов, мультимодальность.

ARTM в свою очередь даёт инструмент для конструирования моделей под любую зачачу. В основе лежит идея регуляризации через KL-дивергенцию(мера похожести вероятностных распределений). Фреймворк включает в себя различные регуляризаторы, которые можно добавлять в модель, чтобы добавлять модели желаемых свойств. Платим мы за это большим количеством коэфициентов, теоретического обоснования для подбора которых пока нет.

Вот [тут](http://www.machinelearning.ru/wiki/images/5/55/Voron-PTM-short.pdf) есть "рекламная" презентация про преимущества ARTM

### Подробнее об ARTM
Достаточно подробная статья находится [здесь](http://www.machinelearning.ru/wiki/images/b/bc/Voron-2015-BigARTM.pdf), но кроме того рекомендую почитать уже номного устаревшую, но более полную уже [упоминавшуюся](http://www.machinelearning.ru/wiki/images/b/bc/Voron-2015-BigARTM.pdf).
Так же нельзя не упомянуть [курс на курсере](https://www.coursera.org/learn/unsupervised-learning/home/welcome), в конце которого есть хорошее введение в тематическое моделирование с практикой.

#### Главное об ARTM: концепция мультимодальности
Давайте внимательнее взглянем на понятие модальности, что это?

Давайте попробуем понять это на примере модели языка.
Итак мысленный эксперимент. Представим себе автора, который пишет какой-то текст. Предположим, это проходная статья на хабр о биоисследованиях в космосе. То есть текст будет состоять на 20% из инженерной темы, на 30% из биологичеческой и на 50% из общей лексики. Сделаем оговорку, что наш автор -- не очень хороший автор: вместо грамотного погружения в тему и последовательного озложения он пишет статьи иначе. Он изготовил трёхгранный "кубик" с площадью сторон 0.2 см^2, 0.3 см^2 и 0.5 см^2 соответственно и каждый раз, когда он собирается добавить новое слово в статью -- он бросает этот кубик, определяя, из какого лексического набора будет очередное слово. Определив это, он берёт новый многогранный(по количеству слов в лексическом наборе, до десятков тысяч граней) кубик(для каждого лексического набора он собственный), бросает его и получает новое слово, которое нужно добавить в текст. Такую процедуру он проделывает около 500 раз, пока не получит текст достаточно внушительной длины.
Звучит, возможно, дико, но это рабочая модель статистического подхода к лингвистике. И что самое главное(и дикое) -- она работает.

#### Модальность
Во-первых, элементы модальности -- это элементы какого-то конечного зафиксированного множества. Например, это множество всех слов в нормальной форме русского языка из словаря Даля. Или множество людей на какой-то конкретный момент времени. Или множество музыкальных треков. Или множество ДНС-серверов.

Во-вторых, для каждого элемента из множества определено число -- вероятность его появления в рамках данной модальности. Сумма этих чисел по всему множеству должна быть равна 1(мы всё-таки с вероятностью работаем).

##### Зачем называть это модальностью? Почему не назвать просто словами?
А это самое интересное. До сих пор мы приводили примеры документов, которые содержат только одну модальность -- токены. Но что если пойти дальше?

###### Пример
Возьмём научные работы. Предположим, что для каждой статьи мы знаем список скачавших эту статью и список авторов. В таком случае, к модальности текстов добавляется еще 2 модальности: прочитавшие статью люди и написавшие статью люди. Сложно не согласиться, что такие люди определяют тематику научной статьи не хуже текста самой статьи.
![big_scheme](./modalities_schema.png)

#### Возвращаемся к ARTM
Чем интересна ARTM -- так это своей естественной возможностью работать с любым количеством модальностей сразу. То есть с помощью ARTM можно получить не одну матрицу фи, а несколько, для каждой модальности отдельно. Причём эти матрицы будут согласованны.

TODO согласованность -- это как? Показать наглядно преобразования.

## Переходим к практике
На текущий момент важно лишь понимание сути модальностей и механики связи модальностей через тематическое пространство

![big_scheme](./big_schema.png)
    - Майнинг данных
      - Источник данных, фильтрация мусора
      - Предподготовка текстов
      - Предподготовка изображений
      - Оговорка о формате данных для ARTM
    - Описание модели
      - Доступные модальности
      - Листинг тематик
    - О поиске
      - Поиск по любой модальности, как это?
    - Дополнительные возможности
      - Как воспользоваться мультимодальностью модели
      - Аннотирование
    - Еще кейсы применения
      - Пользователи ВК
      - Страницы в вебе
      - Научные работы
      - Категоризация
    - Спасибо за внимание, предлагайте кейсы применения мультимодальных моделейных моделей
    - Ссылки
      - ссылка на демо(не шатайте его сильно, пожалуйста)
      - ссылка на BigARTM
      - ссылка на керас
